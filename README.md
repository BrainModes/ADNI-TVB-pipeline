# ADNI-TVB-Pipeline

###### Processing pipeline for multimodal imaging data from the ADNI (Alzheimer's Disease Neuroimaging Initiative) database, to be used as input to The Virtual Brain software.

## Create job submission scripts

The Jupyter notebook parses the dataset, and creates a batch job-submission script for each step of the pipeline.

Each script contains job submission commands for each subject for the given processing step. These commands call the "generic" scripts.

Note: The script assumes jobs will be submitted on an SGE cluster. See the "slurm" branch of this repository if you use this at your institution.

> **Example:**  For the "FreeSurfer" step of the pipeline, the job-submission script generated by the notebook would contain a command for each subject like below:
>
> qsub -o output_\<subID>.txt -e error_\<subID>.txt generic_freesurf.sh /path/to/resultsDir \<subID>

For some scripts, these commands take arguments specifying single-subject parameters that are required for processing.

**Required editing:** Notebook will have to be modified for data paths. The pipeline assumes data is organized according to the BIDS specification. If subjects have scans from multiple visits, the longitudinal data may need to be "flattened" before executing the code in the notebook (see notebook for details).

## Single-subject scripts
The folder "generic_scripts" contains the single subject scripts. Some of these scripts are wrappers for the HCP Pipeline scripts (https://github.com/Washington-University/HCPpipelines), others contain processing commands themselves.

**Required editing:** "Generic" bash scripts may need to be modified for job submission arguments, e.g. queue name.
